{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"7-B138-6FuPo"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24621,"status":"ok","timestamp":1669591576240,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"hC_DkxeQPuzP","outputId":"6eed3ec3-7f4b-4ff8-dca2-51a9f8c3a041"},"outputs":[],"source":["!pip install transformers\n","!pip install gradio\n","!pip install torch\n","!pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!gdown https://drive.google.com/uc?id=1EAjsKML35-b6TseCmmgfkGIq9uqc_oEI"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669587273977,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"3CJszOa9PaEE"},"outputs":[],"source":["import torch\n","from transformers import LongformerForTokenClassification, LongformerTokenizer\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import numpy as np\n","import csv\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1174,"status":"ok","timestamp":1669587275144,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"ZsvXidMMFPEK","outputId":"a3b88986-28f2-4a68-d90d-6c1e1104761d"},"outputs":[],"source":["# use this if on colab and loading the data from google drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669587275144,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"N_fX6FfYwWQB"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","NUM_GPUS = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1279,"status":"ok","timestamp":1669590784929,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"uco7RM11KUvD"},"outputs":[],"source":["unfreeze_range = range(9, 12)\n","learning_rate = 0.001"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ryY1HGdtF3Y1"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4224,"status":"ok","timestamp":1669590790987,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"albpNPboPs7P","outputId":"75070611-1a6e-40a8-9268-a3532468310b"},"outputs":[],"source":["longformer = LongformerForTokenClassification.from_pretrained('allenai/longformer-base-4096')\n","longformer = longformer.to(device)\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1669590790987,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"qxXeft5EAWmQ"},"outputs":[],"source":["def freeze_model(model):\n","  for param in model.parameters():\n","    param.requires_grad = False\n","  for param in model.classifier.parameters():\n","    param.requires_grad = True\n","  for i in unfreeze_range:\n","      for param in model.longformer.encoder.layer[i].parameters():\n","        param.requires_grad = True\n","  return model\n","  \n","longformer = freeze_model(longformer)\n","\n","if NUM_GPUS > 1:\n","  longformer = torch.nn.DataParallel(longformer, list(range(NUM_GPUS)))\n","\n","# only optim last layer\n","optimizer = torch.optim.Adam(longformer.classifier.parameters(), lr=learning_rate)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f3CgoDU_GA5W"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1669590790988,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"SO2Bi9fSkNTg"},"outputs":[],"source":["csv.field_size_limit(sys.maxsize)\n","class DebateDataset(Dataset):\n","    def __init__(self, csv_file, tokenizer, max_len, max_count=-1):\n","        # self.df = pd.read_csv(csv_file)\n","        # self.df = pd.read_csv(csv_file, header=None, sep='\\n')\n","        # df = df[0].str.split('\\s\\|\\s', expand=True)\n","        self.df = []\n","        with open(csv_file, 'r') as f:\n","          reader = csv.reader(f)\n","          count = 0\n","          for row in reader:\n","            count += 1\n","            if count > max_count and max_count != -1:\n","              break\n","            self.df.append([string.replace('\\n', ' ') for string in row])\n","        print(self.df[0])\n","        print(self.df[1])\n","        \n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        row = self.df[index]\n","        abstract = tokenizer(row[0], add_special_tokens=False)['input_ids']\n","        labels = [0] * (len(abstract) + 3)\n","        tokens = []\n","        for i in range(1, len(row), 2):\n","            tokenized = self.tokenizer(row[i+1], add_special_tokens=False)['input_ids']\n","            labels += [int(row[i])] * len(tokenized)\n","            tokens += tokenized\n","        length = 4 + len(abstract) + len(tokens)\n","        \n","        if length > self.max_len:\n","          tokens = tokens[:self.max_len - (4 + len(abstract))]\n","          labels = labels[:self.max_len - 1]\n","        combined = self.tokenizer.build_inputs_with_special_tokens(abstract, tokens)\n","        labels.append(0)\n","        # tokens, attention_mask, global_attention_mask, labels, loss_mask\n","        ret = (\n","             torch.tensor(combined, dtype=torch.int),\n","             torch.tensor([1] * len(combined), dtype=torch.bool),\n","             torch.tensor([1] * (len(abstract)+2) + [0] * (len(tokens) + 2), dtype=torch.bool),\n","             torch.tensor(labels),\n","             torch.tensor([0] * (len(abstract)+2) + [1] * (len(tokens) + 2), dtype=torch.bool)\n","        )\n","        if ret[0].shape != ret[1].shape or ret[0].shape != ret[2].shape or ret[0].shape != ret[3].shape or ret[0].shape != ret[4].shape:\n","          print(ret)\n","          print(ret[0].shape)\n","          print(ret[1].shape)\n","          print(ret[2].shape)\n","          print(ret[3].shape)\n","          print(ret[4].shape)\n","        return ret\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669590790988,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"L3-Eye2hFA0E"},"outputs":[],"source":["def collate(sequences):\n","  tokens, attention_mask, global_attention_mask, labels, loss_mask = list(zip(*sequences))\n","  tokens_batch = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True, padding_value=0)\n","  labels_batch = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n","  attention_mask_batch = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n","  global_attention_mask_batch = torch.nn.utils.rnn.pad_sequence(global_attention_mask, batch_first=True, padding_value=0)\n","  loss_mask_batch = torch.nn.utils.rnn.pad_sequence(loss_mask, batch_first=True, padding_value=0)\n","  return tokens_batch, attention_mask_batch, global_attention_mask_batch, labels_batch, loss_mask_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669591735178,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"rGjlB2EIkarF"},"outputs":[],"source":["# data_loader = torch.utils.data.DataLoader(dataset, batch_size = 2, shuffle = True, collate_fn=collate, num_workers=1)\n","max_len = 2500"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1669590790990,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"ksn1UxWJJmE2","outputId":"e0ed3549-3bcd-418e-b49c-cffa51928be5"},"outputs":[],"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","dataset = DebateDataset('./debatefinal.csv', tokenizer, max_len, max_count=2000)\n","batch_size = 2\n","test_split = .1\n","valid_split = .1\n","shuffle = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","test_index = int(np.floor(test_split * dataset_size))\n","valid_index = int(np.floor(valid_split * dataset_size))\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","test_indices, val_indices, train_indices = indices[:test_index], indices[test_index:test_index+valid_index], indices[test_index + valid_index:]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","test_sampler = SubsetRandomSampler(test_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, collate_fn=collate, num_workers=1, sampler=train_sampler)\n","test_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, collate_fn=collate, num_workers=1, sampler=test_sampler)\n","valid_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, collate_fn=collate, num_workers=1, sampler=valid_sampler)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KS4CzOwlGFBl"},"source":["# Train functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669590790990,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"oV4ShgYO2l_s"},"outputs":[],"source":["cross_entropy = CrossEntropyLoss()\n","def loss_fn(logits, labels, mask):\n","  logits, labels, mask = torch.flatten(logits, 0, 1), torch.flatten(labels), torch.flatten(mask)\n","  logits = logits[mask]\n","  labels = labels[mask]\n","  return cross_entropy(logits, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1669590790990,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"_jrmWNJTHYZk"},"outputs":[],"source":["def accuracy_fn(logits, labels, mask):\n","    logits, labels, mask = torch.flatten(logits, 0, 1), torch.flatten(labels), torch.flatten(mask)\n","    logits = logits[mask]\n","    labels = labels[mask]\n","    truth = logits.max(1)[1].eq(labels)\n","    return truth.sum().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669590790991,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"x_BEya77LUyq"},"outputs":[],"source":["def valid_fn():\n","  longformer.eval()\n","  with torch.no_grad():\n","    total_loss = 0\n","    total_accuracy = 0\n","    total_count = 0\n","    for i, (tokens, attention_mask, global_attention_mask, labels, loss_mask) in enumerate(valid_loader):\n","      tokens = tokens.to(device)\n","      attention_mask = attention_mask.to(device)\n","      global_attention_mask = global_attention_mask.to(device)\n","      labels = labels.to(device)\n","      loss_mask = loss_mask.to(device)\n","\n","      logits = longformer(\n","        tokens, \n","        attention_mask = attention_mask,\n","        global_attention_mask = global_attention_mask\n","      )['logits']\n","\n","      total_loss += loss_fn(logits, labels, loss_mask).item()\n","      total_accuracy += accuracy_fn(logits, labels, loss_mask)\n","      total_count += loss_mask.sum().item()\n","    return total_loss/total_count, total_accuracy/total_count\n","\n","      \n","      \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fvUd5Nu4GIiX"},"source":["# Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":733337,"status":"error","timestamp":1669591525186,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"A-rKTTFlzWHT","outputId":"61366932-c9a6-4638-c371-30d232c8fad3"},"outputs":[],"source":["losses = []\n","accuracies = []\n","epochs = 10\n","for epoch in range(epochs):\n","  for i, (tokens, attention_mask, global_attention_mask, labels, loss_mask) in enumerate(train_loader):\n","    # resets the gradients in the tensors\n","    optimizer.zero_grad()\n","\n","    longformer.train()\n","\n","    tokens = tokens.to(device)\n","    attention_mask = attention_mask.to(device)\n","    global_attention_mask = global_attention_mask.to(device)\n","    labels = labels.to(device)\n","    loss_mask = loss_mask.to(device)\n","\n","    logits = longformer(\n","      tokens, \n","      attention_mask = attention_mask,\n","      global_attention_mask = global_attention_mask\n","    )['logits']\n","\n","    loss = loss_fn(logits, labels, loss_mask)\n","    accuracy = accuracy_fn(logits, labels, loss_mask)/loss_mask.sum().item()\n","    losses.append(loss.item()/loss_mask.sum().item())\n","    accuracies.append(accuracy)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if i % 5 == 0:\n","      print(f'epoch: {epoch}/{epochs} batch: {i} loss: {losses[-1]} accuracy: {accuracies[-1]}') \n","    if i % 50 == 0:\n","      average_loss, average_accuracy = valid_fn()\n","      print(f'validation loss: {average_loss}, validation accuracy: {average_accuracy}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1669081550019,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"WpGI4Z3UC6PG","outputId":"fdb5e210-9d04-40e5-c826-cfbf064b02b6"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(losses)\n","plt.ylim(0, 0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbsD6paHACvE"},"outputs":[],"source":["def predict(model, abstract, text):\n","    abstract = tokenizer(abstract, add_special_tokens=False)['input_ids']\n","    tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n","    length = 4 + len(abstract) + len(tokens)\n","\n","\n","    # if length > self.max_len:\n","    #   tokens = tokens[:self.max_len - (4 + len(abstract))]\n","    #   labels = labels[:self.max_len - 1]\n","    combined = tokenizer.build_inputs_with_special_tokens(abstract, tokens)\n","    # tokens, attention_mask, global_attention_mask, labels, loss_mask\n","\n","    global_attention_mask = torch.tensor([1] * (len(abstract)+2) + [0] * (len(tokens) + 2), dtype=torch.bool).unsqueeze(0).to(device)\n","    tokens = torch.tensor(combined, dtype=torch.int).unsqueeze(0).to(device)\n","    attention_mask = torch.tensor([1] * len(combined), dtype=torch.bool).unsqueeze(0).to(device)\n","    \n","    model.eval()\n","    logits = model(\n","        tokens, \n","        attention_mask = attention_mask,\n","        global_attention_mask = global_attention_mask\n","    )['logits']\n","    mask = torch.logical_and(logits.max(2)[1], torch.logical_not(global_attention_mask))\n","\n","    print(mask)\n","    for (token, highlight) in zip(tokens[0], mask[0]):\n","        if highlight:\n","            print('\\033[31m' + tokenizer.decode(token) + '\\033[0m', end='')\n","        else:\n","            print(tokenizer.decode(token), end='')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":629,"status":"ok","timestamp":1669085117217,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"73hc3w2JAOiG","outputId":"c53a1dd5-c1db-461c-8591-3103f4c8e686"},"outputs":[],"source":["predict(longformer, 'Privacy violations inevitable – tech and corporations',\n","'The truth is that consumers love the benefits of digital goods and are willing to give up traditionally private information in exchange for the manifold miracles that the Internet and big data bring. Apple and Android each offer more than a million apps, most of which are built upon this model, as are countless other Internet services. More generally, big data promises huge improvements in economic efficiency and productivity, and in health care and safety. Absent abuses on a scale we have not yet seen, the public’s attitude toward giving away personal information in exchange for these benefits will likely persist, even if the government requires firms to make more transparent how they collect and use our data. One piece of evidence for this is that privacy-respecting search engines and email services do not capture large market shares. In general these services are not as easy to use, not as robust, and not as efficacious as their personal-data-heavy competitors. Schneier understands and discusses all this. In the end his position seems to be that we should deny ourselves some (and perhaps a lot) of the benefits big data because the costs to privacy and related values are just too high. We “have to stop the slide” away from privacy, he says, not because privacy is “profitable or efficient, but because it is moral.” But as Schneier also recognizes, privacy is not a static moral concept. “Our personal definitions of privacy are both cultural and situational,” he acknowledges. Consumers are voting with their computer mice and smartphones for more digital goods in exchange for more personal data. The culture increasingly accepts the giveaway of personal information for the benefits of modern computerized life. This trend is not new. “The idea that privacy can’t be invaded at all is utopian,” says Professor Charles Fried of Harvard Law School. “There are amounts and kinds of information which previously were not given out and suddenly they have to be given out. People adjust their behavior and conceptions accordingly.” That is Fried in the 1970 Newsweek story, responding to an earlier generation’s panic about big data and data mining. The same point applies today, and will apply as well when the Internet of things makes today’s data mining seem as quaint as 1970s-era computation.'\n","      )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_SFaa6UTXuVF"},"source":["# Gradio"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":648},"executionInfo":{"elapsed":168122,"status":"ok","timestamp":1669593300092,"user":{"displayName":"Julian Bauer","userId":"01667424859645158169"},"user_tz":480},"id":"_M2OVnkHXxde","outputId":"5ed9b147-aa9e-4d5c-f5ae-a246fc7b74f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://f1932d6819c4f2bd40.gradio.live\n"]},{"data":{"text/plain":[]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import gradio as gr\n","\n","def predict_gradio(abstract, text, threshold):\n","    abstract = tokenizer(abstract, add_special_tokens=False)['input_ids']\n","    tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n","    length = 4 + len(abstract) + len(tokens)\n","\n","\n","    if length > max_len:\n","      return \"TOO LONG\"\n","    combined = tokenizer.build_inputs_with_special_tokens(abstract, tokens)\n","\n","    global_attention_mask = torch.tensor([1] * (len(abstract)+2) + [0] * (len(tokens) + 2), dtype=torch.bool).unsqueeze(0).to(device)\n","    tokens = torch.tensor(combined, dtype=torch.int).unsqueeze(0).to(device)\n","    attention_mask = torch.tensor([1] * len(combined), dtype=torch.bool).unsqueeze(0).to(device)\n","    \n","    longformer.eval()\n","    logits = longformer(\n","        tokens, \n","        attention_mask = attention_mask,\n","        global_attention_mask = global_attention_mask\n","    )['logits']\n","    probs = torch.nn.functional.softmax(logits, dim=2)\n","    mask = torch.logical_and(probs[:, :, 1] > threshold, torch.logical_not(global_attention_mask))\n","\n","    ret = ''\n","    for (token, highlight) in zip(tokens[0], mask[0]):\n","        if highlight:\n","            ret += '<span class=\"highlight\">' + tokenizer.decode(token) + '</span>'\n","        else:\n","            ret += tokenizer.decode(token)\n","    print(ret)\n","    return ret.split('</s></s>')[1]\n","\n","with gr.Blocks(css=\".highlight {background-color: yellow; color: black}\") as demo:\n","    abstract = gr.Textbox(label=\"Tag\")\n","    text = gr.Textbox(label=\"Text\")\n","    threshold = gr.Slider(label=\"Threshold\", minimum=0, maximum=1, step=0.01)\n","    highlighted = gr.Markdown(label=\"Highlighted\")\n","    greet_btn = gr.Button(\"Magic\")\n","    greet_btn.click(fn=predict_gradio, inputs=[abstract, text, threshold], outputs=highlighted)\n","\n","demo.launch(share=True, debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
